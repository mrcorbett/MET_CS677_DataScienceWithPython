{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    " - Feature scaling through standardization (or Z-score normalization)\n",
    "can be an important preprocessing step for many machine learning\n",
    "algorithms. \n",
    " - Standardization involves rescaling the features such\n",
    "that they have the properties of a standard normal distribution\n",
    "with a mean of zero and a standard deviation of one.\n",
    "- While many algorithms (such as SVM, K-nearest neighbors, and logistic\n",
    "regression) require features to be normalized, intuitively we can\n",
    "think of Principle Component Analysis (PCA) as being a prime example\n",
    "of when normalization is important. \n",
    "- In PCA we are interested in the\n",
    "components that maximize the variance. \n",
    "- If one component (e.g. human\n",
    "height) varies less than another (e.g. weight) because of their\n",
    "respective scales (meters vs. kilos), PCA might determine that the\n",
    "direction of maximal variance more closely corresponds with the\n",
    "'weight' axis, if those features are not scaled. \n",
    "- As a change in\n",
    "height of one meter can be considered much more important than the\n",
    "change in weight of one kilogram, this is clearly incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Features\")\n",
    "print(load_wine().feature_names)\n",
    "\n",
    "print(\"\\nClasses\")\n",
    "print(load_wine().target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, target = load_wine(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, target, test_size=0.30, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit to data and predict using pipelined GNB and PCA.\n",
    "\n",
    "unscaled_clf = make_pipeline(PCA(n_components=2), GaussianNB())\n",
    "unscaled_clf.fit(X_train, y_train)\n",
    "\n",
    "pred_test = unscaled_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit to data and predict using pipelined scaling, GNB and PCA.\n",
    "\n",
    "std_clf = make_pipeline(StandardScaler(),\n",
    "                        PCA(n_components=2), GaussianNB())\n",
    "std_clf.fit(X_train, y_train)\n",
    "\n",
    "pred_test_std = std_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show prediction accuracies in scaled and unscaled data.\n",
    "\n",
    "print('\\nPrediction accuracy for the normal test dataset with PCA')\n",
    "print('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_test)))\n",
    "\n",
    "print('\\nPrediction accuracy for the standardized test dataset with PCA')\n",
    "print('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_test_std)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PCA from pipeline\n",
    "\n",
    "pca = unscaled_clf.named_steps['pca']\n",
    "\n",
    "pca_std = std_clf.named_steps['pca']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first principal components\n",
    "\n",
    "print('\\nPC 1 without scaling:\\n', pca.components_[0])\n",
    "\n",
    "print('\\nPC 1 with scaling:\\n', pca_std.components_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PCA without and with scale on X_train data for visualization.\n",
    "\n",
    "X_train_transformed = pca.transform(X_train)\n",
    "\n",
    "scaler = std_clf.named_steps['standardscaler']\n",
    "X_train_std_transformed = pca_std.transform(scaler.transform(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize standardized vs. untouched dataset with PCA performed\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 7))\n",
    "\n",
    "\n",
    "for l, c, m in zip(range(0, 3), ('blue', 'red', 'green'), ('^', 's', 'o')):\n",
    "    ax1.scatter(X_train_transformed[y_train == l, 0],\n",
    "                X_train_transformed[y_train == l, 1],\n",
    "                color=c,\n",
    "                label='class %s' % l,\n",
    "                alpha=0.5,\n",
    "                marker=m\n",
    "                )\n",
    "\n",
    "for l, c, m in zip(range(0, 3), ('blue', 'red', 'green'), ('^', 's', 'o')):\n",
    "    ax2.scatter(X_train_std_transformed[y_train == l, 0],\n",
    "                X_train_std_transformed[y_train == l, 1],\n",
    "                color=c,\n",
    "                label='class %s' % l,\n",
    "                alpha=0.5,\n",
    "                marker=m\n",
    "                )\n",
    "\n",
    "ax1.set_title('Training dataset after PCA')\n",
    "ax2.set_title('Standardized training dataset after PCA')\n",
    "\n",
    "for ax in (ax1, ax2):\n",
    "    ax.set_xlabel('1st principal component')\n",
    "    ax.set_ylabel('2nd principal component')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid()\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
